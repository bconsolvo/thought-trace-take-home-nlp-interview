
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pytesseract approach using only the PDF first\n",
    "First, I use pytesseract to extract the text from the PDF only, and then use one of Pytesseract's functions to get a 0-100 confidence value for each word. Although I am not making use of the ThoughtTrace-provided word library, I do get a confidence value for each word. The confidence value obtained here uses the algorithm developed in Google's Tessaract library. Some of the technical papers outlining the Tesseract OCR method can be found here:\n",
    "https://github.com/tesseract-ocr/tesseract/wiki/Technical-Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing required Python libraries for code if needed\n",
    "# !pip install wand\n",
    "# !python -m pip install --upgrade pip\n",
    "# Installed pytesseract from notes on StackExchange: \n",
    "# https://stackoverflow.com/questions/48357030/pytesseract-output-is-not-defined\n",
    "# !pip install tesseract\n",
    "# !pip install tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries \n",
    "from wand.image import Image as Img\n",
    "try:  \n",
    "    from PIL import Image\n",
    "except ImportError:  \n",
    "    import Image\n",
    "import pytesseract\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytesseract import Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes necessary to point the command for tesseract to the installed location on a Windows machine:\n",
    "# pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files (x86)\\\\Tesseract-OCR\\\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF to JPG - divides into a JPG file for every page of the PDF\n",
    "with Img(filename='doc_test.pdf', resolution=300) as img:\n",
    "    img.compression_quality = 99\n",
    "    img.save(filename='doc_test.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PIL to open the image, and use pytesseract to transcribe the image to a text string.\n",
    "# Must have pytesseract and tesseract properly installed, and have the proper command for opening tesseract\n",
    "# Making a list of all jpg files if I wanted to use a for-loop over all of my jpg files\n",
    "im_list = glob.glob('doc_test-*.jpg') \n",
    "im_list = sorted(im_list)\n",
    "# Function to extract text from an image using pytesseract\n",
    "def ocr_1(filename):\n",
    "    im = Image.open(filename) # opening the image with PIL Image function\n",
    "    text = pytesseract.image_to_string(im) # Uses Tesseract to convert JPG image to a string of text\n",
    "    scores = pytesseract.image_to_data(im, output_type=pytesseract.Output.DATAFRAME) # To provide extracted data about each word extracted from the page.\n",
    "    b = scores[['text','conf']] #Extracting only the text and confidence columns only (each word has a confidence from 0-100)\n",
    "    b2 = b.conf.replace(-1,np.NaN) # Replacing -1 confidence values with NaNs, so that they are ignored\n",
    "    mean_b2 = b2.mean() # Taking the mean confidence value of page 1 of the PDF (now with NaNs excluded from mean)\n",
    "    print('The mean confidence value of', filename, 'is', mean_b2,'.')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running OCR on all 5 images from the PDF and storing text in a list.\n",
    "lst_scores = []\n",
    "for i in im_list:\n",
    "    df_text = ocr_1(i)\n",
    "    lst_scores.append(df_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest confidence value is for 'doc_test-3.jpg' at around 78/100, which is the 4th page of the PDF, and one with the least amount of text. This page has signatures, and harder-to-read text, and so we would expect lower confidence. <br> <b>These confidence values have nothing to do with the confidence values that I develop in my function - they are confidence values from Google's Tesseract function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the text to a dataframe, and then writing out a CSV to be used in the confidence function later.\n",
    "df = pd.DataFrame(lst_scores)\n",
    "df.to_csv('pyt_output.csv',index=False,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing my own confidence function using doc_test_ocr.csv extracted OCR text and valid_words.txt dictionary\n",
    "Now, I write my own confidence function based on the text outputs provided.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, the best accuracy measure of a document of text would first be to manually write out the entire document, and ensure that the manually-written document has 0 mistakes; and then compare the manually-copied document with whatever output of OCR. This comparison could be done character by character, word-by-word, paragraph-by-paragraph, or even the whole document. \n",
    "<br>\n",
    "<br>\n",
    "However, to manually write every document out to test its accuracy would take significant effort and time, and would not be feasible for many documents, and with many pages of text. \n",
    "<br>\n",
    "<br>\n",
    "Instead, we can use a confidence measure from the word library provided. By cross-referencing the words found in the OCR output with the words in the provided dictionary, we can assign confidence values to each word, and ultimately, to the document as a whole. We cannot assume that this confidence value is necessarily a measure of accuracy, as we do not have a pure \"true\" copy of the document's data (text). But the aim here would be to come up with a confidence measure that would reflect the quality of the PDF scanned document.\n",
    "<br> \n",
    "<br> \n",
    "Assumptions:\n",
    "- The word dictionary provided contains all of the words that would be written in the PDF document.\n",
    "- Comparing the OCR output with the provided dictionary is a good measure of scan quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_all(ocr_csv,dct_csv):\n",
    "    #### Reading in the OCR output and words dictionary ####\n",
    "    ocr_o = pd.read_csv(ocr_csv,names = [\"Text\"]) # Each line is a new paragraph in this OCR output.\n",
    "    dct_o = pd.read_csv(dct_csv,names = [\"Words\"]) # Reading in the dictionary of words\n",
    "    dct_o = dct_o['Words'].tolist() # Make the words dictionary dataframe into a list.\n",
    "    #ocr_o.head() # display first 5 lines of the transcribed paragraphs\n",
    "    \n",
    "    #### PREPROCESSING OF THE TRANSCRIPTION OUTPUT ####\n",
    "    '''\n",
    "    Because the provided word dictionary only contains lower case words, with no numbers, and no punctuation, \n",
    "    we must preprocess the CSV file before evaluating it to match the word dictionary. We must:\n",
    "        1. Remove all punctuation\n",
    "        2. Remove all digits\n",
    "        3. Replace more than 1 space with 1 space\n",
    "    '''\n",
    "    ocr_lc = pd.DataFrame(ocr_o['Text'].str.lower()) # Put all words in lower case\n",
    "    ocr_lc['Text'] = ocr_lc['Text'].str.replace('[^\\w\\s]','') # remove all punctuation from text string\n",
    "    ocr_lc['Text'] = ocr_lc['Text'].str.replace('_','') # removes all underscores _ from text \n",
    "    ocr_lc['Text'] = ocr_lc['Text'].str.replace('\\d+', '') # remove all numbers\n",
    "    ocr_lc['Text'] = ocr_lc['Text'].str.strip() # removes leading and end soaces from strings\n",
    "    \n",
    "    # For loop to reduce multiple spaces to 1 space for all strings\n",
    "    lst_0 = []\n",
    "    for i in ocr_lc['Text']:\n",
    "        a = ' '.join(i.split())\n",
    "        lst_0.append(a)\n",
    "    se_1 = pd.Series(lst_0)\n",
    "    ocr_lc['Text'] = pd.Series(lst_0)\n",
    "    '''\n",
    "    ### Not currently used\n",
    "    ### Optional for loop to limit word length to eliminate short 1-2 letter words from OCR output\n",
    "    lst_1 = []\n",
    "    for i in se_1:\n",
    "        b = ' '.join( [w for w in i.split() if len(w)>2] )\n",
    "        lst_1.append(b)\n",
    "    lst_1\n",
    "    '''\n",
    "    #### Write out the preprocessed text file ####\n",
    "    out_csv = ocr_csv.split('.')[-2] + '_preproc.csv'\n",
    "    ocr_lc.to_csv(out_csv,index=False) #Write out the preprocessed CSV file for QC\n",
    "    ocr_lc.head() # Display top 5 lines of preprocessed text\n",
    "    \n",
    "    #### Calculating statistics on words and putting into a master dataframe ####\n",
    "    ocr_master = pd.DataFrame(ocr_lc['Text'].str.split()) # split words in each row by commas to make lists\n",
    "    l1 = ocr_master['Text'].tolist() # converting the dataframe column Text into a list type\n",
    "    word_totals = list(map(len,l1)) # getting the length of each row of list of words\n",
    "    se = pd.Series(word_totals) # converting list to series\n",
    "    ocr_master['word_totals'] = se.values # inserting the word totals into the dataframe ocr_split\n",
    "    #ocr_master.head() # Displaying first 5 lines of new dataframe.\n",
    "    ser1 = ocr_master['Text']\n",
    "    lst1 = []\n",
    "    for i in ser1:\n",
    "        countw = len(set(i) & set(dct_o)) #Key line for comparing words in each row to the words in the dictionary\n",
    "        lst1.append(countw) # Putting the count into a list\n",
    "    se_wc = pd.Series(lst1) # Making the lst1 with the count into a series before putting into the dataframe\n",
    "    \n",
    "    ocr_master['matching_words'] = se_wc.values # Adding the number of matching words as a column in the master dataframe\n",
    "    ocr_master['d'] = ocr_master['matching_words'] / ocr_master['word_totals'] # A straight percentage grade for each paragraph detected\n",
    "    \n",
    "    ocr_master['all_words'] = ocr_master['word_totals'].sum() # Total of all words\n",
    "    ocr_master['weight'] = ocr_master['word_totals'] / ocr_master['all_words'] # Weight for each set of words\n",
    "    ocr_master['weighted_grade'] = ocr_master['d'] * ocr_master['weight'] # Weighted grade for each set of words\n",
    "    #ocr_master # Displaying the new data-frame with the new statistics columns\n",
    "    final_grade = 100 * ocr_master['weighted_grade'].sum() # Adding all of the weighted grades for a final score\n",
    "    final_grade = round(final_grade) # final confidence out of 100 for the whole document\n",
    "    return ocr_master, final_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function on the ThoughtTrace provided output OCR CSV file with the provided (valid_words.txt) dictionary\n",
    "df_master_1, confidence_1 = ocr_all('doc_test_ocr.csv','valid_words.txt')\n",
    "print('The scanned image has a confidence of',confidence_1,'out of 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the function on the pytesseract output .csv file with the provided (valid_words.txt) dictionary\n",
    "df_master_2, confidence_2 = ocr_all('pyt_output.csv','valid_words.txt')\n",
    "print('The scanned image has a confidence of',confidence_2,'out of 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "- Many transcribed words were not found in dictionary, and thus the confidence function might seem abnormally low (around 50% confidence). \n",
    "- With a more comprehensive words dictionary, the score would improve\n",
    "- However, there were also a number of words transcribed that were not words in the English language, which would decrease the score. <br> For example:\n",
    "-- \"ailic\", \"pes\" \"cps\", \"sm\", \"oc\"\n",
    "- The OCR output provided from ThoughtTrace performed better at 49 / 100 than the output from my crude pytesseract OCR (36 / 100).\n",
    "- The confidence measure on the scan quality of the PDF is really a measure of the quality of the OCR output, and if the words actually appear in the valid_words.txt words dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ThoughtTrace provided OCR output score table\n",
    "df_master_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytesseract output score table\n",
    "df_master_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
